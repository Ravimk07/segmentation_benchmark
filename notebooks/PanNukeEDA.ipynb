{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from catalyst import dl, metrics, core, contrib, utils\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is EDA of PanNuke dataset (https://jgamper.github.io/PanNukeDataset/). Data is split in 3 folds, each stored as separate .npy file; 19 tissues in total, 5 nuclei classes, ~200,000 labeled nuclei; masks are stored in individual channels (OHE encoded: neoplastic, non-neoplastic epithelial, inflammatory, connective, dead, background); This dataset also contains instance segmentation of each nuclei , which we are not going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load('../data/PanNuke/images/fold1_images.npy')\n",
    "masks = np.load('../data/PanNuke/masks/fold1_masks.npy')\n",
    "types = np.load('../data/PanNuke/types/fold1_types.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_examples(images, masks, types, n_plot = 6):\n",
    "    f, ax = plt.subplots(n_plot, n_plot, figsize=(4*n_plot, 4*n_plot))\n",
    "    ax = ax.flatten()\n",
    "    idx_choice = np.random.choice(images.shape[0], size=n_plot**2)\n",
    "    for idx, idx_plot in enumerate(idx_choice):\n",
    "        ax[idx].imshow(images[idx_plot].astype(int))\n",
    "        ax[idx].imshow(np.argmax(masks[idx_plot].astype(int), axis=2), alpha=0.5, cmap='Accent', vmin=0, vmax=5)\n",
    "        ax[idx].set_title(types[idx_plot])\n",
    "        ax[idx].axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_examples(images, masks, types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest case: we can train the model to segment nuclei into one of the 5 classes (+background), just an example of multiclass semantic segmentation. We can test different models (for example, Unet, Unet++, FPN, Linknet, PSPNet, PAN) with different encoders (different variations of ResNet, EfficientNet, RegNet, ResNest, etc) and different augmentation techniques (D4, scaling, crops, cutouts, cutmix, etc). Testing different LR and optimizers will be too much, so we may just use RAdam + Lookahead, since they perform OK and don't require rigorous LR + scheduler tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Catalyst, since it works OK, easy to use and allows logging results, saving multiple checkpoints, have nice callbacks and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with defining datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "\n",
    "class PanNukeDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images,\n",
    "        masks,\n",
    "        types,\n",
    "            transforms):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.types = types\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Will load the mask, get random coordinates around/with the mask,\n",
    "        load the image by coordinates\n",
    "        \"\"\"\n",
    "        sample_image = self.images[idx]\n",
    "        sample_mask = np.argmax(self.masks[idx].astype(int), axis=2)\n",
    "        #sample_mask = (self.masks[idx] > 0).astype(int)\n",
    "        sample_type = self.types[idx]\n",
    "        augmented = self.transforms(image=sample_image, mask=sample_mask)\n",
    "        sample_image = augmented['image']\n",
    "        sample_image = sample_image.transpose(2, 0, 1)  # channels first\n",
    "        #sample_mask = sample_mask.transpose(2, 0, 1)  # channels first\n",
    "        #sample_mask = np.expand_dims(augmented['mask'], 0)\n",
    "\n",
    "        data = {'features': torch.from_numpy(sample_image.copy()).float(),\n",
    "                'mask': torch.from_numpy(sample_mask.copy())}\n",
    "        return(data)\n",
    "    \n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Normalize()\n",
    "        ],\n",
    "        p=1.0)\n",
    "\n",
    "def light_training_transforms():\n",
    "    return A.Compose([\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Transpose(),\n",
    "                A.VerticalFlip(),\n",
    "                A.HorizontalFlip(),\n",
    "                A.RandomRotate90(),\n",
    "                A.NoOp()\n",
    "            ], p=1.0),\n",
    "        A.Normalize()\n",
    "    ])\n",
    "\n",
    "def medium_training_transforms():\n",
    "    return A.Compose([\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Transpose(),\n",
    "                A.VerticalFlip(),\n",
    "                A.HorizontalFlip(),\n",
    "                A.RandomRotate90(),\n",
    "                A.NoOp()\n",
    "            ], p=1.0),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                GridMask(num_grid=6),\n",
    "                A.CoarseDropout(max_holes=16, max_height=16, max_width=16),\n",
    "                A.NoOp()\n",
    "            ], p=1.0),\n",
    "        A.Normalize()\n",
    "    ])\n",
    "\n",
    "\n",
    "def heavy_training_transforms():\n",
    "    return A.Compose([\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Transpose(),\n",
    "                A.VerticalFlip(),\n",
    "                A.HorizontalFlip(),\n",
    "                A.RandomRotate90(),\n",
    "                A.NoOp()\n",
    "            ], p=1.0),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ElasticTransform(),\n",
    "                A.GridDistortion(),\n",
    "                A.OpticalDistortion(),\n",
    "                A.NoOp(),\n",
    "                A.ShiftScaleRotate(),\n",
    "            ], p=1.0),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.GaussNoise(),\n",
    "                A.GaussianBlur(),\n",
    "                A.NoOp()\n",
    "            ], p=1.0),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.CLAHE(),\n",
    "                A.RGBShift(),\n",
    "                A.RandomBrightnessContrast(),\n",
    "                A.RandomGamma(),\n",
    "                A.HueSaturationValue(),\n",
    "                A.NoOp()\n",
    "            ], p=1.0),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                GridMask(num_grid=6),\n",
    "                A.CoarseDropout(max_holes=16, max_height=16, max_width=16),\n",
    "                A.NoOp()\n",
    "            ], p=1.0),\n",
    "        A.Normalize()\n",
    "    ])\n",
    "\n",
    "def get_training_trasnforms(transforms_type):\n",
    "    if transforms_type == 'light':\n",
    "        return(light_training_transforms())\n",
    "    elif transforms_type == 'medium':\n",
    "        return(medium_training_transforms())\n",
    "    elif transforms_type == 'heavy':\n",
    "        return(heavy_training_transforms())\n",
    "    else:\n",
    "        raise NotImplementedError(\"Not implemented transformation configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-load data\n",
    "images = np.load('../data/PanNuke/images/fold1_images.npy')\n",
    "masks = np.load('../data/PanNuke/masks/fold1_masks.npy')\n",
    "types = np.load('../data/PanNuke/types/fold1_types.npy')\n",
    "\n",
    "\n",
    "images_val = np.load('../data/PanNuke/images/fold2_images.npy')\n",
    "masks_val = np.load('../data/PanNuke/masks/fold2_masks.npy')\n",
    "types_val = np.load('../data/PanNuke/types/fold2_types.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PanNukeDataset(images, masks, types, get_training_trasnforms('light'))\n",
    "val_dataset = PanNukeDataset(images_val, masks_val, types_val, get_valid_transforms())\n",
    "\n",
    "loaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=8, shuffle=True),\n",
    "    'valid': DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_toolbelt.losses import DiceLoss, \n",
    "from pytorch_toolbelt.utils.catalyst import IoUMetricsCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus('resnet18', classes=6)\n",
    "model.cuda()\n",
    "learning_rate = 0.001\n",
    "encoder_learning_rate = 0.0005\n",
    "layerwise_params = {\"encoder*\": dict(lr=encoder_learning_rate, weight_decay=0.00003)}\n",
    "model_params = utils.process_model_params(model, layerwise_params=layerwise_params)\n",
    "base_optimizer = contrib.nn.RAdam(model_params, lr=learning_rate, weight_decay=0.0003)\n",
    "optimizer = contrib.nn.Lookahead(base_optimizer)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.25, patience=2)\n",
    "criterion = {\n",
    "    \"dice\": DiceLoss(mode='multiclass'),\n",
    "    \"ce\": nn.CrossEntropyLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import  CriterionCallback, MetricAggregationCallback\n",
    "\n",
    "callbacks = [\n",
    "    # Each criterion is calculated separately.\n",
    "    CriterionCallback(\n",
    "       input_key=\"mask\",\n",
    "        prefix=\"loss_dice\",\n",
    "        criterion_key=\"dice\"\n",
    "    ),\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_ce\",\n",
    "        criterion_key=\"ce\"\n",
    "    ),\n",
    "\n",
    "    # And only then we aggregate everything into one loss.\n",
    "    MetricAggregationCallback(\n",
    "        prefix=\"loss\",\n",
    "        mode=\"weighted_sum\", \n",
    "        metrics={\n",
    "            \"loss_dice\": 1.0, \n",
    "            \"loss_ce\": 0.8\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # metrics\n",
    "    IoUMetricsCallback(\n",
    "        mode='multiclass', \n",
    "        input_key='mask', \n",
    "        class_names=[\n",
    "            'neoplastic', \n",
    "            'non-neoplastic epithelial', \n",
    "            'inflammatory', \n",
    "            'connective',\n",
    "            'dead',\n",
    "            'background'\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = dl.SupervisedRunner(input_key=\"features\", input_target_key=\"mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    callbacks=callbacks,\n",
    "    logdir='../logs/initial_test',\n",
    "    num_epochs=3,\n",
    "    main_metric=\"loss\",\n",
    "    minimize_metric=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a train.py which will run thru all our models\\encoders, something like\n",
    "https://github.com/rwightman/pytorch-image-models/blob/master/train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation_models",
   "language": "python",
   "name": "segmentation_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
